# import findspark
import numpy as np
import pandas as pd

# findspark.init()
from pyspark.sql.types import *
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import os

# import matplotlib.pyplot as plt

"""
TODO:

 ASK:
 1. do we load enoght ?
 2. do our maskanot mast be realated to PRCP
 3. can we hold seperate table for each maskena ?
 4. does our code should show the plots or its on another notebook
 5. should we keep the data of our maskanot in the DB server ? or can we show graph on the fly?
 6. how can we overrite TOO LONG time of waiting ? cluster is not faster!
 7. what prediction shhould we show ? prcp where ? how ? and when ?
 
1. Data Analysis:
    a. Derive a temporal-based insight:
        * 
    b. Derive a spatial-based insight:
        * the country with highest precipitation
    c. Derive a spatio-temporal-based insight:
        * for each continent, plot the yearly average precipitation

Core variables: PRCP, SNOW, SNWD, TMAX, TMIN
Irrelevant variables: AWDR, AWND, DAWM, FMTM, FRGB, FRGT, FRTH, GAHT, MDWM, PGTM, THIC, WDF1, WDF2, WDF5, WDFG, WDFI,
WDFM, WDMV, WSF1, WSF2, WSF5, WSFG, WSFI, WSFM, 

Precipitation can be predicted by observing past precipitation records, max/min temperatures, snowfall, cloudiness.
Wind records may be irrelevant.

"""


def init_spark(app_name: str):
    spark = SparkSession.builder.appName(app_name).getOrCreate()
    sc = spark.sparkContext
    return spark, sc


def StaticStream():
    os.environ['PYSPARK_SUBMIT_ARGS'] = "--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1," \
                                        "com.microsoft.azure:spark-mssql-connector_2.12:1.1.0 pyspark-shell"
    spark, sc = init_spark('RoiAndYarin')
    kafka_server = 'dds2020s-kafka.eastus.cloudapp.azure.com:9092'
    noaa_schema = StructType([StructField('StationId', StringType(), False),
                              StructField('Date', IntegerType(), False),
                              StructField('Variable', StringType(), False),
                              StructField('Value', IntegerType(), False),
                              StructField('M_Flag', StringType(), True),
                              StructField('Q_Flag', StringType(), True),
                              StructField('S_Flag', StringType(), True),
                              StructField('ObsTime', StringType(), True)])
    kafka_raw_df = spark \
        .read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", kafka_server) \
        .option("subscribe", "CH") \
        .option("startingOffsets", "earliest") \
        .load()
    kafka_value_df = kafka_raw_df.selectExpr("CAST(value AS STRING)")
    json_df = kafka_value_df.select(F.from_json(F.col("value"), schema=noaa_schema).alias('json'))
    # Flatten the nested object:
    kafka_df = json_df.select("json.*")
    return os, spark, sc, kafka_df


def ReadDf(spark, tableName):
    os.environ['PYSPARK_SUBMIT_ARGS'] = "--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1," \
                                        "com.microsoft.azure:spark-mssql-connector_2.12:1.1.0 pyspark-shell"
    server_name = "jdbc:sqlserver://technionddscourse.database.windows.net"
    database_name = "yarinbs"
    url = server_name + ";" + "databaseName=" + database_name + ";"

    table_name = tableName
    username = "yarinbs"
    password = "Qwerty12!"  # Please specify password here
    format_str = "com.microsoft.sqlserver.jdbc.spark"
    format_str2 = "jdbc"
    jdbcDF = spark.read \
        .format("com.microsoft.sqlserver.jdbc.spark") \
        .option("url", url) \
        .option("dbtable", table_name) \
        .option("user", username) \
        .option("password", password).load()
    return jdbcDF




def readStreamData(spark, flag="false"):
    os.environ['PYSPARK_SUBMIT_ARGS'] = "--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1," \
                                        "com.microsoft.azure:spark-mssql-connector_2.12:1.1.0 pyspark-shell"
    # spark, sc = init_spark('RoiAndYarin')
    kafka_server = 'dds2020s-kafka.eastus.cloudapp.azure.com:9092'
    noaa_schema = StructType([StructField('StationId', StringType(), False),
                              StructField('Date', IntegerType(), False),
                              StructField('Variable', StringType(), False),
                              StructField('Value', IntegerType(), False),
                              StructField('M_Flag', StringType(), True),
                              StructField('Q_Flag', StringType(), True),
                              StructField('S_Flag', StringType(), True),
                              StructField('ObsTime', StringType(), True)])
    if (flag):
        streaming_input_df = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_server) \
            .option("subscribe", "EZ, US") \
            .option("startingOffsets", "earliest") \
            .load() \
            .limit(100000000) \
            .selectExpr("CAST(value AS STRING)") \
            .select(F.from_json(F.col("value"), schema=noaa_schema).alias('json')) \
            .select("json.*") \
            .drop("M_Flag", "S_Flag", "ObsTime") \
            .filter(F.col("Q_Flag").isNull()) \
            .drop("Q_Flag") \
            .withColumn("fullDate", F.to_date(F.concat(F.col("Date")), "yyyyMMdd")) \
            .drop("Date")
    else:
        streaming_input_df = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_server) \
            .option("subscribe", "EZ, US") \
            .option("startingOffsets", "earliest") \
            .load() \
            .limit(100000) \
            .selectExpr("CAST(value AS STRING)") \
            .select(F.from_json(F.col("value"), schema=noaa_schema).alias('json')) \
            .select("json.*") \
            .drop("M_Flag", "S_Flag", "ObsTime") \
            .filter(F.col("Q_Flag").isNull()) \
            .drop("Q_Flag") \
            .withColumn("fullDate", F.to_date(F.concat(F.col("Date")), "yyyyMMdd")) \
            .drop("Date")\
            


    query = streaming_input_df \
        .writeStream \
        .outputMode("append") \
        .foreachBatch(lambda df, epoch_id: writeToserver(df, "WeatherTestTable2")) \
        .trigger(processingTime='60 seconds') \
        .start()
    query.awaitTermination()
    print("finished")


def writeToserver(df, tableName):
	df.groupBy(['Stationid', 'Year']).pivot('Variable').agg(F.avg("value")).orderBy("Year") 
    os.environ['PYSPARK_SUBMIT_ARGS'] = "--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1," \
                                        "com.microsoft.azure:spark-mssql-connector_2.12:1.1.0 pyspark-shell"
    server_name = "jdbc:sqlserver://technionddscourse.database.windows.net"
    database_name = "yarinbs"
    url = server_name + ";" + "databaseName=" + database_name + ";"

    table_name = tableName
    username = "yarinbs"
    password = "Qwerty12!"  # Please specify password here
    format_str = "com.microsoft.sqlserver.jdbc.spark"
    format_str2 = "jdbc"

    try:
        df.write \
            .format(format_str2) \
            .mode("append") \
            .option("url", url) \
            .option("dbtable", table_name) \
            .option("user", username) \
            .option("password", password) \
            .save()
    except ValueError as error:
        print("Connector write failed", error)


def transformChangePerContinent(df, GeoData="null"):
    # Df 'Date' column transformation
    # df = df.withColumn("fullDate", F.to_date(F.concat(F.col("Date")), "yyyyMMdd"))
    df = df.withColumn("Year", F.year("fullDate"))
    df = df.withColumn('FIPS', F.substring(df['StationId'], 0, 2))
    df = df.select(['FIPS', 'Year', 'Variable', 'Value', 'Q_Flag'])
    df = df.filter(df.Q_Flag.isNull())  # Remove rows where Q_Flag is not Null
    df = df.filter(df.Variable == "PRCP").limit(300000)
    data_joined = df.join(GeoData, on=['FIPS'])
    data_joined = data_joined.groupBy(['continent', 'year']).agg(F.avg('Value').alias("PRCP")).orderBy("year")
    data_joined = data_joined.repartitionByRange(4, F.col("continent"))
    ChangePerContinent = {}
    # ChangePerContinent["Europe"] = data_joined.filter(data_joined.continent == "Europe").toPandas()
    # ChangePerContinent["Asia"] = data_joined.filter(data_joined.continent == "Asia").toPandas()
    # ChangePerContinent['Africa'] = data_joined.filter(data_joined.continent == "Africa").toPandas()
    # ChangePerContinent['Oceania'] = data_joined.filter(data_joined.continent == "Oceania").toPandas()
    # ChangePerContinent['Americas'] = data_joined.filter(data_joined.continent == "Americas").toPandas()
    continents = data_joined.select('continent').distinct().tolist()
    for c in continents:
        ChangePerContinent[c] = data_joined.filter(data_joined.continent == c)
        writeToserver(ChangePerContinent[c], c + "PRCPPerYear")
    # # df = df.groupBy(['StationId', 'Year']).pivot('Variable').avg('Value')
    # df = df.filter(df.PRCP.isNotNull())  # Remove rows where Q_Flag is not Null
    # dfAVGperYear = df.withColumn("Year", F.year("fullDate"))
    # dfAVGperYear = dfAVGperYear.groupBy("year").agg(F.avg("PRCP").alias("AvgPrcp"))
    # data_joined = df.join(GeoData, on=['FIPS'])
    # data_joined.show(100)
    # return ChangePerContinent


def temporal_insight(df):
    print(df.count())
    # Temporal insight - global max/min temperature trends

    # df1 = df.select(['Year', 'Variable', 'Value']).where("Variable = PRCP")
    # df1 = df1.groupby(['Year']).agg(F.avg('Value').alias("Yearly Average Precipitation")).orderBy("Year")
    # AvgPRCPdf = df1.select(['Year', 'Value']).toPandas()
    # plt.plot(AvgPRCPdf, label="Average")

    # df = df.withColumn("fullDate", F.to_date(F.concat(F.col("Date")), "yyyyMMdd"))
    df = df.withColumn("Year", F.year("fullDate"))
    df = df.select(['Year', 'Variable', 'Value'])
    df = df.filter("Variable == 'TMAX' or Variable == 'TMIN'").limit(300000)
    df = df.groupBy(['Year']).pivot('Variable').agg(F.avg("value")).orderBy("Year")
    # df2 = df2.filter(df.TMIN.isNotNull() & df.TMAX.isNotNull()).limit(3000)
    df = df.withColumn('Gap', F.col("TMAX") - F.col("TMIN"))
    # df2 = df2.groupby(['Year']).agg(F.max('Gap'))
    savetoDB = df.select(['Year', 'Gap'])
    writeToserver(savetoDB, "world_year_Temp_diff")
    MaxPRCPdf = savetoDB.toPandas()
    plt.plot(MaxPRCPdf['Year'], MaxPRCPdf['Gap'], label="Maximum")

    # df3 = df.select(['Year', 'Variable', 'Value']).filter(df.Variable == "TMIN").limit(5000)
    # df3 = df3.groupby(['Year']).agg(F.min('Value').alias("min")).orderBy("Year")
    # MinPRCPdf = df3.select(['Year', 'min']).toPandas()
    # plt.plot(MinPRCPdf['Year'], MinPRCPdf['min'], label="Minimum")

    plt.xlabel('Years')
    plt.ylabel('AvgMaxTemp-AvgMinTemp')
    plt.title('Annual Global Maximum-Minimum Temperature Trends')
    plt.legend()
    plt.show()


def spatial_insight(df):
    pass


def make_first_graph(dict):
    # x = np.arange(-4,2*np.pi, 0.3)
    # y = 2*np.sin(x)
    # y2 = 3*np.cos(x)
    for key in dict.keys():
        x = dict[key]["year"]
        y = dict[key]["PRCP"]
        plt.plot(dict[key]["year"], dict[key]["PRCP"], '-gD')
        for xitem, yitem in np.nditer([x, y]):
            etiqueta = "{:.1f}".format(xitem)
            # plt.annotate(etiqueta, (xitem, yitem), textcoords="offset points", xytext=(0, 10), ha="center")
    plt.grid(True)
    plt.show()


def snowTransform(kafka_df):
    Dfsnow = kafka_df.select(['FIPS', 'Year', 'Variable', 'Value', 'Q_Flag'])
    Dfsnow = Dfsnow.filter("Variable ==SNOW").limit(300000)


def main():
    os, spark, sc, kafka_df = StaticStream()
    spark, sc = init_spark('RoiAndYarin')
    # print(initiateServer())
    readStreamData(spark)
    # GeoData = ReadDf(spark, tableName="GeoData")
    # dict1 = transformChangePerContinent(kafka_df, GeoData)
    # make_first_graph(dict1)
    # temporal_insight(kafka_df)

    # writeToserver(tDf, "AvgPRCPPerYeat")


if __name__ == '__main__':
    main()
